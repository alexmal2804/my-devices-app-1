Создай в данном приложении ссуже реализованным чатом "AI-помощник" с  LLM моделью, функционал, который использует
для более точных ответов модели  RAG LangChain  и векторную базу, в которую можно загружать документы в формате PDF, DOCX, XLSX, CSV, и TXT.
Подключи векторую базу Supabase к LangChain, чтобы она могла использоваться для поиска документов и извлечения информации из них.
Параметры подключения к Supabase  указаны в файле .env.local, который уже существует в проекте:
VITE_NEXT_PUBLIC_SUPABASE_URL
VITE_NEXT_PUBLIC_SUPABASE_ANON_KEY
В составе приложения интерфейса с использованием Material UI реализуй окно формирования векторной базы с возможностью
загрузки файлов c локального компьютера. Модальное окно загрузки докментов в базу открываетя о нажатии кнопки "Загрузить документы", расположенной
справа в верхней части окна приложения
Локальные данные хранятся Redux Toolkit и передаются в Supabase для создания векторной базы. При загрузке документов, они должны быть обработаны и сохранены в Supabase.
После загрузки документов, пользователь может задавать вопросы в чате, и модель будет использовать  RAG LangChain для поиска и извлечения информации из загруженных документов и предоставления более точных ответов.
Максимально используй уже реализованный функционал приложения, чтобы не дублировать код и не создавать лишние зависимости.
Максимально аккуратно интегрируй новый функционал в существующий код, чтобы не нарушить работу приложения и не создать конфликтов с уже существующими компонентами.
// Для реализации RAG LangChain встрой его в уже имеющийся пайп LangChain, которая позволяет создавать цепочки обработки данных и интегрировать их с LLM моделями.
// Для работы с векторной базой используй библиотеку Supabase, которая позволяет хранить и обрабатывать векторные данные.